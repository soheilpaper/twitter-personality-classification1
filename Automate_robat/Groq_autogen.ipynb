{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/twitter-personality-classification1/blob/master/Automate_robat/Groq_autogen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-agentchat~=0.2"
      ],
      "metadata": {
        "id": "9u2c1bpBC3T-",
        "outputId": "c2a40c7d-6755-41bc-e6d3-714f0755e049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-agentchat~=0.2\n",
            "  Downloading autogen_agentchat-0.2.40-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting diskcache (from autogen-agentchat~=0.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from autogen-agentchat~=0.2)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from autogen-agentchat~=0.2)\n",
            "  Downloading FLAML-2.3.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (1.57.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (24.2)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (2.10.3)\n",
            "Collecting python-dotenv (from autogen-agentchat~=0.2)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (2.5.0)\n",
            "Collecting tiktoken (from autogen-agentchat~=0.2)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat~=0.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat~=0.2) (2.27.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen-agentchat~=0.2) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen-agentchat~=0.2) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen-agentchat~=0.2) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen-agentchat~=0.2) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen-agentchat~=0.2) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat~=0.2) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat~=0.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat~=0.2) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen-agentchat~=0.2) (3.4.0)\n",
            "Downloading autogen_agentchat-0.2.40-py3-none-any.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.3/382.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.3-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.2/314.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, flaml, diskcache, tiktoken, docker, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.2.40 diskcache-5.6.3 docker-7.1.0 flaml-2.3.3 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from autogen import AssistantAgent, UserProxyAgent\n",
        "\n",
        "llm_config = { \"config_list\": [{ \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\") }] }\n",
        "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)\n",
        "\n",
        "# Start the chat\n",
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"Tell me a joke about NVDA and TESLA stock prices.\",\n",
        ")"
      ],
      "metadata": {
        "id": "7ZP1GJVsC1UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from autogen import AssistantAgent, UserProxyAgent\n",
        "\n",
        "# Configure Groq API\n",
        "llm_config = {\n",
        "    \"config_list\": [\n",
        "        {\n",
        "            \"model\": \"llama3-8b-8192\",  # Example model supported by Groq\n",
        "            \"api_key\": \"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\", #os.environ.get(\"GROQ_API_KEY\"),\n",
        "            \"base_url\": \"https://api.groq.com/openai/v1\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Initialize agents\n",
        "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)\n",
        "\n",
        "# Start the chat\n",
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"Tell me a joke about NVDA and TESLA stock prices.\"\n",
        ")"
      ],
      "metadata": {
        "id": "ujWgU5fYD_7x",
        "outputId": "31adf291-2f75-426f-9f37-d1838751f1cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Tell me a joke about NVDA and TESLA stock prices.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 12-25 09:08:18] {351} WARNING - Model llama3-8b-8192 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:autogen.oai.client:Model llama3-8b-8192 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant (to user_proxy):\n",
            "\n",
            "Sounds like a challenging task! Let's gather some information first.\n",
            "\n",
            "To make a joke about NVDA and TESLA stock prices, I'd like to know the current stock prices of both companies. I'll print the current stock prices and then use them to craft a joke.\n",
            "\n",
            "Please execute the following Python code:\n",
            "```python\n",
            "# filename: get_stock_prices.py\n",
            "import requests\n",
            "\n",
            "nvidia_response = requests.get('https://www alpha vantage.co/query?function=GLOBAL_QUOTE&symbol=NVDA&apikey=YOUR_API_KEY')\n",
            "tesla_response = requests.get('https://www alpha vantage.co/query?function=GLOBAL_QUOTE&symbol=TSLA&apikey=YOUR_API_KEY')\n",
            "\n",
            "print(nvidia_response.json()['Global Quote']['05. price'])\n",
            "print(tesla_response.json()['Global Quote']['05. price'])\n",
            "```\n",
            "Replace `YOUR_API_KEY` with your actual API key from Alpha Vantage or a similar service.\n",
            "\n",
            "After executing the code, you should see the current stock prices of NVDA and TESLA printed. Now, I can use these prices to craft a joke for you!\n",
            "\n",
            "Please provide the output of the stock prices, and I'll do my best to come up with a joke that's both funny and relevant to NVDA and TESLA stock prices.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as user_proxy. Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: Do it\n",
            "user_proxy (to assistant):\n",
            "\n",
            "Do it\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 12-25 09:10:55] {351} WARNING - Model llama3-8b-8192 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model llama3-8b-8192 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant (to user_proxy):\n",
            "\n",
            "Let me execute the code to get the current stock prices of NVDA and TESLA. Please provide the output of the code.\n",
            "\n",
            "**Please note:** You'll need to replace `YOUR_API_KEY` with your actual Alpha Vantage API key. If you don't have one, you can sign up for a free account.\n",
            "\n",
            "After you provide the output, I'll craft a joke using the stock prices and share it with you!\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}